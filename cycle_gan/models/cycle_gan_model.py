import torch
import itertools
from util.image_pool import ImagePool
from .base_model import BaseModel
from . import networks


class CycleGANModel(BaseModel):
	"""
	è¯¥ç±»å®ç° CycleGAN æ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯éæˆå¯¹æ•°æ®é›†çš„ image-to-image ç¿»è¯‘ã€‚

	æ¨¡å‹è®­ç»ƒéœ€è¦ä½¿ç”¨ '--dataset_mode unaligned' æ•°æ®é›†å®ä¾‹.
	é»˜è®¤ä½¿ç”¨ '--netG resnet_9blocks' ResNet ç”Ÿæˆå™¨,
	'--netD basic' åˆ¤åˆ«å™¨ (pix2pix ä¸­å¼•å…¥çš„ PatchGAN),
	æœ€å°å¹³æ–¹ GANs çº¦æŸ ('--gan_mode lsgan').

	CycleGAN åŸæ–‡: https://arxiv.org/pdf/1703.10593.pdf
	"""
	@staticmethod
	def modify_commandline_options(parser, is_train=True):
		"""åŠ å…¥æ–°çš„ ç‰¹å®šæ•°æ®é›†å‘½ä»¤è¡Œå‚æ•°ï¼ˆdataset-specific optionsï¼‰ï¼Œå¹¶é‡æ–°æŒ‡å®šå·²çŸ¥å‚æ•°é€‰é¡¹çš„å€¼ã€‚

		Parameters:
			parser			åŸå§‹å‘½ä»¤è¡Œé€‰é¡¹è§£é‡Šå™¨
			is_train (bool) -- è®­ç»ƒ/æµ‹è¯• é˜¶æ®µã€‚å¯ä»¥ä½¿ç”¨è¯¥æ ‡å¿—å†å¢åŠ  ç‰¹å®šçš„è®­ç»ƒ/æµ‹è¯•é€‰é¡¹ã€‚

		Returns: ä¿®æ”¹åçš„å‘½ä»¤è¡Œé€‰é¡¹è§£é‡Šå™¨

		CycleGANä¸­é™¤äº† GAN losses, æˆ‘ä»¬è¿˜å¼•å…¥äº† lambda_A, lambda_B, å’Œ lambda_identity ä¸‰ä¸ªæƒé‡æ¥åŠ æƒä¸‹é¢å‡ ä¸ªæŸå¤±ã€‚
		A (åŸå§‹åŸŸ), B (ç›®æ ‡åŸŸ).
		ç”Ÿæˆå™¨: G_A: A -> B; G_B: B -> A.
		åˆ¤åˆ«å™¨: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.
		å‰å‘ cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)
		åå‘ cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)
		Identity loss (å¯é€‰): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 "Photo generation from paintings" in the paper)
		åŸå§‹ CycleGAN è®ºæ–‡æ²¡æœ‰ä½¿ç”¨ dropoutã€‚
		"""
		parser.set_defaults(no_dropout=True)  # CycleGAN é»˜è®¤æ²¡æœ‰ä½¿ç”¨ dropout
		if is_train:
			parser.add_argument('--lambda_A', type=float, default=10.0, help='cycle loss (A -> B -> A) çš„æƒé‡')
			parser.add_argument('--lambda_B', type=float, default=10.0, help='cycle loss (B -> A -> B) çš„æƒé‡')
			parser.add_argument('--lambda_identity', type=float, default=0.5, help='identity æ˜ å°„. å°† lambda_identity è®¾ä¸ºé 0 å¯ä»¥è°ƒæ•´ identity æ˜ å°„çš„ lossã€‚ä¸¾ä¸ªğŸŒ°ï¼Œå¦‚æœ identity loss æ˜¯ reconstruction loss çš„ååˆ†ä¹‹ä¸€ï¼Œåˆ™è®¾ç½® lambda_identity = 0.1')

		return parser

	def __init__(self, opt):
		""" åˆå§‹åŒ– CycleGAN ç±».

		Parameters:
			opt (Option ç±»)-- å®éªŒçš„å‘½ä»¤è¡Œé€‰é¡¹ï¼Œéœ€ç»§æ‰¿è‡ª BaseOptions
		"""
		BaseModel.__init__(self, opt)
		# æŒ‡å®šéœ€è¦æ‰“å°çš„è®­ç»ƒæŸå¤±ï¼Œè®­ç»ƒ/æµ‹è¯• è„šæœ¬ä¼šè°ƒç”¨ BaseModel.get_current_losses æ¥å¾—åˆ° losses çš„å€¼
		self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']
		# æŒ‡å®šéœ€è¦ ä¿å­˜/å±•ç¤º çš„å›¾ç‰‡ã€‚ è®­ç»ƒ/æµ‹è¯• è„šæœ¬ä¼šè°ƒç”¨ BaseModel.get_current_visuals æ¥å¾—åˆ° ä¿å­˜/å±•ç¤º çš„å›¾ç‰‡
		visual_names_A = ['real_A', 'fake_B', 'rec_A']
		visual_names_B = ['real_B', 'fake_A', 'rec_B']
		if self.isTrain and self.opt.lambda_identity > 0.0:  # å¦‚æœ identity loss ä½¿ç”¨ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹ idt_B=G_A(B), idt_A=G_A(B) ä¸¤ä¸ªæŸå¤±å¯è§†åŒ–
			visual_names_A.append('idt_B')
			visual_names_B.append('idt_A')

		self.visual_names = visual_names_A + visual_names_B
		# æŒ‡å®šéœ€è¦ä¿å­˜åˆ°ç¡¬ç›˜çš„æ¨¡å‹ï¼Œè®­ç»ƒ/æµ‹è¯• è„šæœ¬ä¼šè°ƒç”¨ BaseModel.save_networks å’Œ BaseModel.load_networks ä¸¤ä¸ªæ–¹æ³•
		if self.isTrain:
			self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']
		else:  # æµ‹è¯•é˜¶æ®µä»…åŠ è½½ ç”Ÿæˆå™¨
			self.model_names = ['G_A', 'G_B']

		# å®šä¹‰ç½‘ç»œï¼šç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨
		# è¿™é‡Œç½‘ç»œåå­—å’Œæ–‡ç« ä¸­çš„åå­—ä¸åŒã€‚ä»£ç ï¼ˆVS æ–‡ç« ï¼‰: G_A (G), G_B (F), D_A (D_Y), D_B (D_X)
		self.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,
										not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)
		self.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,
										not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)

		if self.isTrain:  # å®šä¹‰åˆ¤åˆ«å™¨
			self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,
											opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)
			self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,
											opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)

		if self.isTrain:
			if opt.lambda_identity > 0.0:  # ä»…å½“è¾“å…¥ä¸è¾“å‡ºçš„å›¾ç‰‡æœ‰åŒæ ·é€šé“æ•°çš„æ—¶å€™æ‰ work(é€šé“æ•°ä¸ä¸€æ ·è¿˜æ€ä¹ˆæ¯”è¾ƒå•Šï¼)
				assert(opt.input_nc == opt.output_nc)
			self.fake_A_pool = ImagePool(opt.pool_size)  # åˆ›å»ºå›¾ç‰‡ç¼“å†²æ± æ¥å­˜å‚¨å…ˆå‰ç”Ÿæˆçš„å›¾ç‰‡
			self.fake_B_pool = ImagePool(opt.pool_size)
			# å®šä¹‰ loss function
			self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # å®šä¹‰ GAN loss
			self.criterionCycle = torch.nn.L1Loss()
			self.criterionIdt = torch.nn.L1Loss()
			# åˆå§‹åŒ– optimizers; çˆ¶ç±» BaseModel.setup ä¼šè‡ªåŠ¨åˆ›å»º schedulers
			self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
			self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
			self.optimizers.append(self.optimizer_G)
			self.optimizers.append(self.optimizer_D)

	def set_input(self, input):
		""" å°† dataloader ä¸­æ•°æ®è§£åŒ…å¹¶è¿›è¡Œæ•°æ®é¢„å¤„ç†

		Parameters:
			input (dict): æ•°æ®ä¸æ•°æ®ä¿¡æ¯
			é€‰é¡¹ 'direction' å¯ç”¨äºäº¤æ¢ A/B åŸŸ
		"""
		AtoB = self.opt.direction == 'AtoB'
		self.real_A = input['A' if AtoB else 'B'].to(self.device)
		self.real_B = input['B' if AtoB else 'A'].to(self.device)
		self.image_paths = input['A_paths' if AtoB else 'B_paths']

	def forward(self):
		"""å‰å‘è¿ç®—ã€‚è¢« optimize_parameters å’Œ test ä¸¤ä¸ªå‡½æ•°è°ƒç”¨"""
		self.fake_B = self.netG_A(self.real_A)  # G_A(A)
		self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))
		self.fake_A = self.netG_B(self.real_B)  # G_B(B)
		self.rec_B = self.netG_A(self.fake_A)   # G_A(G_B(B))

	def backward_D_basic(self, netD, real, fake):
		""" è®¡ç®—åˆ¤åˆ«å™¨çš„ GAN loss

		Parameters:
			netD (network)	  -- åˆ¤åˆ«å™¨D
			real (tensor array) -- çœŸå®å›¾ç‰‡
			fake (tensor array) -- ç”Ÿæˆå™¨ç”Ÿæˆçš„å›¾ç‰‡

		è¿”å› åˆ¤åˆ«å™¨loss
		è¿™é‡Œä¹Ÿè°ƒç”¨ loss_D.backward() æ¥è®¡ç®—æ¢¯åº¦
		"""
		# çœŸå®å›¾ç‰‡
		pred_real = netD(real)
		loss_D_real = self.criterionGAN(pred_real, True)
		# ç”Ÿæˆå›¾ç‰‡
		pred_fake = netD(fake.detach())
		loss_D_fake = self.criterionGAN(pred_fake, False)
		# ç»“åˆä¸¤ä¸ª loss å¹¶è®¡ç®—æ¢¯åº¦
		loss_D = (loss_D_real + loss_D_fake) * 0.5
		loss_D.backward()
		return loss_D

	def backward_D_A(self):
		""" è®¡ç®— åˆ¤åˆ«å™¨ D_Açš„ GAN loss"""
		fake_B = self.fake_B_pool.query(self.fake_B)
		self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)

	def backward_D_B(self):
		""" è®¡ç®— åˆ¤åˆ«å™¨ D_Bçš„ GAN loss"""
		fake_A = self.fake_A_pool.query(self.fake_A)
		self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)

	def backward_G(self):
		""" è®¡ç®—ç”Ÿæˆå™¨ G_A å’Œ G_B çš„ loss"""
		lambda_idt = self.opt.lambda_identity
		lambda_A = self.opt.lambda_A
		lambda_B = self.opt.lambda_B
		# Identity loss
		if lambda_idt > 0:
			# å¦‚æœreal_Bé€å…¥G_Aç½‘ç»œçš„è¯ï¼ŒG_A åº”è¯¥å’ŒåŸå§‹ real_B ä¸€æ¨¡ä¸€æ ·ã€‚||G_A(B) - B||
			self.idt_A = self.netG_A(self.real_B)
			self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt
			# å¦‚æœreal_Aé€å…¥G_Bç½‘ç»œçš„è¯ï¼ŒG_B åº”è¯¥å’ŒåŸå§‹ real_A ä¸€æ¨¡ä¸€æ ·ã€‚||G_B(A) - A||
			self.idt_B = self.netG_B(self.real_A)
			self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt
		else:
			self.loss_idt_A = 0
			self.loss_idt_B = 0

		# GAN loss D_A(G_A(A))
		self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)
		# GAN loss D_B(G_B(B))
		self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)
		# å‰å‘ cycle loss || G_B(G_A(A)) - A||
		self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A
		# åå‘ cycle loss || G_A(G_B(B)) - B||
		self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B
		# é›†åˆ loss å¹¶è®¡ç®— æ¢¯åº¦
		self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B
		self.loss_G.backward()

	def optimize_parameters(self):
		""" è®¡ç®—æŸå¤±ï¼Œæ¢¯åº¦ï¼Œæ›´æ–°ç½‘ç»œæƒé‡ï¼›æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­è¢«è°ƒç”¨ """
		# forward
		self.forward()	  # è®¡ç®—â€œå‡â€ï¼ˆç”Ÿäº§çš„ï¼‰çš„å›¾ç‰‡å¹¶é‡å»ºå›¾ç‰‡
		# G_A and G_B
		self.set_requires_grad([self.netD_A, self.netD_B], False)  # å½“æ›´æ–°ç”Ÿæˆå™¨çš„æ—¶å€™ï¼Œåˆ¤åˆ«å™¨ä¸éœ€è¦æ›´æ–°æ¢¯åº¦
		self.optimizer_G.zero_grad()  # å°†ç”Ÿæˆå™¨G_A å’Œ G_B æ¢¯åº¦æ¸…é›¶
		self.backward_G()			 # å°†ç”Ÿæˆå™¨ G_A and G_B çš„æ¢¯åº¦åå‘ä¼ å¯¼
		self.optimizer_G.step()	   # æ›´æ–°ç”Ÿæˆå™¨ G_A and G_B çš„æƒé‡
		# D_A and D_B
		self.set_requires_grad([self.netD_A, self.netD_B], True)
		self.optimizer_D.zero_grad()   # å°†åˆ¤åˆ«å™¨ D_A and D_B çš„æ¢¯åº¦æ¸…é›¶
		self.backward_D_A()	  # å¯¹åˆ¤åˆ«å™¨ D_A è®¡ç®—æ¢¯åº¦
		self.backward_D_B()	  # å¯¹åˆ¤åˆ«å™¨ D_B è®¡ç®—æ¢¯åº¦
		self.optimizer_D.step()  # æ›´æ–°åˆ¤åˆ«å™¨ D_A and D_B çš„æƒé‡
